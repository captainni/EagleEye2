# EagleEye2 测试场景指南

本文档详细说明各功能模块的测试步骤和预期结果验证点。

## 1. 爬虫任务执行测试

### 测试步骤

1. **登录后台**
   - 访问 http://localhost:8088
   - 输入用户名密码
   - 点击登录

2. **进入爬虫管理**
   - 导航到"爬虫管理"页面
   - 查看任务列表

3. **创建新任务**
   - 点击"新建任务"按钮
   - 选择爬取类型（政策/竞品）
   - 配置爬取参数
   - 提交任务

4. **监控任务执行**
   - 查看任务状态变化
   - 刷新任务列表查看进度
   - 检查日志输出

5. **验证爬取结果**
   - 进入文章列表页面
   - 确认新文章已爬取
   - 检查文章内容完整性

### 验证点

- [ ] 任务创建成功
- [ ] 任务状态正确更新（待执行→执行中→已完成）
- [ ] 爬取文章数量符合预期
- [ ] 文章内容完整（标题、正文、来源、时间）

---

## 2. 竞品/政策分析测试

### 竞品分析测试

1. **查看竞品文章列表**
   - 进入"竞品追踪"页面
   - 测试筛选条件（公司、时间范围、重要性）
   - 验证排序功能

2. **分析单篇文章**
   - 点击文章查看详情
   - 验证分析结果展示（摘要、关键要点、相关度等）
   - 测试"转需求"功能

3. **批量操作**
   - 选择多篇文章
   - 批量转需求
   - 验证需求生成正确

### 政策分析测试

1. **查看政策文章列表**
   - 进入"政策监控"页面
   - 测试筛选条件（政策类型、领域、时间范围）
   - 验证搜索功能

2. **分析政策影响**
   - 查看政策详情
   - 验证影响分析展示
   - 测试"转需求"功能

### 验证点

- [ ] 列表筛选/搜索功能正常
- [ ] 文章详情完整展示
- [ ] 分析结果包含所有必需字段
- [ ] 转需求功能正常工作

---

## 3. 前端页面交互测试

### 导航测试

1. **主菜单导航**
   - 点击各菜单项（仪表盘、政策监控、竞品追踪、需求池、爬虫管理）
   - 验证页面正确跳转
   - 检查面包屑导航

2. **页面刷新**
   - 各页面按 F5 刷新
   - 验证状态保持正确

### 表单交互

1. **搜索表单**
   - 输入搜索关键词
   - 选择筛选条件
   - 点击搜索/重置

2. **提交表单**
   - 填写必填项
   - 验证表单校验
   - 提交成功提示

### 数据展示

1. **列表分页**
   - 测试翻页功能
   - 验证每页条数切换

2. **数据加载**
   - 首次加载数据
   - 下拉刷新
   - 加载状态显示

### 验证点

- [ ] 所有页面可正常访问
- [ ] 表单校验正确触发
- [ ] 分页功能正常
- [ ] 加载状态正确显示

---

## 4. 完整端到端测试

### 爬取→分析→需求转化流程

1. **创建爬虫任务**
   - 新建政策/竞品爬取任务
   - 等待任务完成

2. **查看爬取结果**
   - 进入文章列表
   - 确认新文章已爬取

3. **执行分析**
   - 对文章进行分析
   - 查看分析结果

4. **转化需求**
   - 选择文章转需求
   - 填写需求信息
   - 提交到需求池

5. **验证需求池**
   - 进入需求池页面
   - 确认需求已生成
   - 验证需求内容正确

### 验证点

- [ ] 完整流程无错误
- [ ] 数据流转正确
- [ ] 需求信息完整准确
- [ ] 所有关联数据正确

---

## 通用验证点

### 功能性

- [ ] 所有按钮可点击且响应正确
- [ ] 所有表单可提交
- [ ] 所有数据可正确加载
- [ ] 错误提示友好明确

### 性能

- [ ] 页面加载时间 < 3秒
- [ ] 操作响应时间 < 1秒
- [ ] 大数据量列表可正常展示

### 兼容性

- [ ] Chrome 浏览器正常
- [ ] 不同分辨率下布局正常
- [ ] 前后端 API 对接无误

---

## 5. 完整爬取分析端到端测试

### 测试目标
验证从触发爬虫任务到查看分析结果的完整数据流转链路。

### 前置条件
- 所有服务正常运行（MySQL、后端、前端、Proxy Service）
- 已登录系统（产品经理或管理员角色）
- 爬虫配置已存在（如"金融界银行动态"）

### 测试步骤

#### 步骤 1：触发爬虫任务
1. 导航到"爬虫管理"页面
2. 确认在"配置管理" Tab 页
3. 找到"金融界银行动态"配置记录
4. 点击该记录的【立即触发】按钮（运行图标 ▶）
5. 观察按钮状态变化

**预期结果**：
- 按钮点击后有反馈（如 loading 状态或提示消息）
- 无错误提示

#### 步骤 2：监控爬取执行
1. 切换到"任务监控" Tab 页
2. 在筛选器中设置：
   - 配置名称 = "金融界银行动态"
   - 时间范围 = "今天"
3. 点击"查询"按钮
4. 观察任务列表中是否出现新任务
5. 同时打开终端监控日志：
   ```bash
   tail -f logs/backend.log | grep -E "爬取|crawler|fetch"
   ```
6. 刷新页面直到任务状态显示"已完成"

**预期结果**：
- 任务记录出现在列表顶部
- 任务状态：待执行 → 执行中 → 已完成
- 爬取状态：成功
- 文章数量 > 0
- 日志显示爬取完成且有文章数量

#### 步骤 3：触发智能分析
1. 在任务监控列表中，找到刚完成的任务记录
2. 点击该记录的【智能分析】按钮
3. 继续监控日志：
   ```bash
   tail -f logs/backend.log | grep -E "分析|analyze|claude"
   ```
4. 等待分析完成（约 10-30 秒）
5. 刷新页面直到分析状态显示"已分析"

**预期结果**：
- 分析按钮状态变化（loading 或禁用）
- 分析状态更新为"已分析"
- 日志显示分析成功完成

#### 步骤 4：验证分析结果 - 监管政策页面
1. 导航到"监管政策"页面
2. 设置筛选条件：
   - 时间范围 = "今天"
   - 排序 = "时间降序"
3. 查看列表顶部是否有新记录
4. 点击新记录查看详情
5. 验证详情页面的所有字段：
   - 标题：正确显示文章标题
   - 来源：显示来源网站
   - 时间：显示爬取时间
   - 政策类型：正确分类（如"规范性文件"、"部门规章"等）
   - 重要程度：显示等级（高/中/低）
   - 相关度：显示等级（高/中/低）
   - 摘要：内容完整且有意义
   - 关键条款：列出重要条款
   - 影响分析：分析内容合理
   - 可执行建议：提供具体建议

**预期结果**：
- 有新卡片生成（如果爬取内容属于政策类）
- 卡片信息完整
- 详情页所有字段显示正常
- 分析内容质量符合要求

#### 步骤 5：验证分析结果 - 竞品动态页面
1. 导航到"竞品动态"页面
2. 设置筛选条件：
   - 时间范围 = "今天"
   - 排序 = "时间降序"
3. 查看列表顶部是否有新记录
4. 点击新记录查看详情
5. 验证详情页面的所有字段：
   - 标题：正确显示文章标题
   - 竞品公司：识别正确
   - 动态类型：分类准确（如"营销活动"、"产品发布"等）
   - 重要性：显示等级（高/中/低）
   - 相关度：显示等级（高/中/低）
   - 摘要：内容完整
   - 关键要点：列出要点
   - 市场影响：分析合理
   - 竞争态势：分析到位
   - 可执行建议：建议具体可行

**预期结果**：
- 有新卡片生成（如果爬取内容属于竞品类）
- 卡片信息完整
- 详情页所有字段显示正常
- 分析内容质量符合要求

### 验证检查清单

#### 数据流转
- [ ] 爬虫任务成功创建
- [ ] 爬取状态正确更新
- [ ] 分析任务成功执行
- [ ] 结果正确保存到数据库
- [ ] 前端正确展示结果

#### 数据质量
- [ ] 文章标题完整
- [ ] 来源信息准确
- [ ] 时间戳正确
- [ ] 分析字段完整（政策类型/竞品公司、重要性、相关度等）
- [ ] 摘要内容有意义
- [ ] 关键条款/要点准确
- [ ] 影响分析合理
- [ ] 建议具有可操作性

### 日志监控指南

#### 爬取日志关注
```bash
tail -f logs/backend.log | grep -E "爬取|crawler|fetch|article"
```
关键信息：
- 爬取开始/结束时间
- 成功爬取的文章数量
- 错误信息（如有）

#### 分析日志关注
```bash
tail -f logs/backend.log | grep -E "分析|analyze|claude|ai"
```
关键信息：
- 分析任务开始
- Claude API 调用状态
- 分析结果保存成功

### 常见问题排查

#### 问题 1：任务监控列表没有新任务
**排查步骤**：
1. 检查是否切换到了"任务监控" Tab
2. 刷新页面
3. 检查筛选条件是否正确
4. 查看后端日志是否有错误

#### 问题 2：爬取状态显示"失败"
**排查步骤**：
1. 查看"错误信息"列的详细错误
2. 检查 Proxy Service 是否运行：`curl http://localhost:8000/health`
3. 检查 Claude CLI 日志：`tail -f logs/claude-cli.log`
4. 检查目标网站是否可访问

#### 问题 3：分析状态卡在"分析中"
**排查步骤**：
1. 检查后端日志中的 API 调用情况
2. 确认 Proxy Service 正常
3. 查看是否有超时错误
4. 尝试点击"再分析"按钮重新触发

#### 问题 4：结果页面没有新卡片
**排查步骤**：
1. 确认分析状态为"已分析"
2. 检查筛选条件（时间范围、关键词等）
3. 刷新页面
4. 直接通过搜索功能查找文章标题
